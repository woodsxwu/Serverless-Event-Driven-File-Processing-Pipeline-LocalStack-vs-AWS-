# Complete Workflow Diagrams

Visual guides for understanding and using the data ingestion pipeline.

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              USER / APPLICATION                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ 1. Upload CSV File
                             â”‚    (employee_data.csv)
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     S3 Bucket        â”‚
                    â”‚   ğŸ“¦ uploads/        â”‚â—„â”€â”€â”€ Event Source
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ 2. S3 Event Notification
                               â”‚    ObjectCreated:*.csv
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Lambda Function    â”‚
                    â”‚   âš¡ processor       â”‚
                    â”‚                      â”‚
                    â”‚  â€¢ Fetch CSV         â”‚
                    â”‚  â€¢ Parse rows        â”‚
                    â”‚  â€¢ Infer schema      â”‚
                    â”‚  â€¢ Compute stats     â”‚
                    â”‚  â€¢ Check quality     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                             â”‚
                â”‚ 3. Upload Summary           â”‚ 4. Write Metadata
                â–¼                             â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   S3 Bucket      â”‚         â”‚   DynamoDB       â”‚
       â”‚   ğŸ“„ processed/  â”‚         â”‚   ğŸ—„ï¸ Metadata    â”‚
       â”‚                  â”‚         â”‚                  â”‚
       â”‚ Summary JSON:    â”‚         â”‚ Record:          â”‚
       â”‚ â€¢ file_name      â”‚         â”‚ â€¢ file_name âš¡   â”‚
       â”‚ â€¢ schema         â”‚         â”‚ â€¢ schema         â”‚
       â”‚ â€¢ statistics     â”‚         â”‚ â€¢ statistics     â”‚
       â”‚ â€¢ quality issues â”‚         â”‚ â€¢ quality issues â”‚
       â”‚ â€¢ timestamps     â”‚         â”‚ â€¢ status         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â€¢ timestamps     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                             â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ 5. Logs & Metrics
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   CloudWatch         â”‚
                    â”‚   ğŸ“Š Monitoring      â”‚
                    â”‚                      â”‚
                    â”‚  â€¢ Lambda logs       â”‚
                    â”‚  â€¢ Performance       â”‚
                    â”‚  â€¢ Error tracking    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

```
CSV File Upload
      â”‚
      â”œâ”€â–º S3 stores file in uploads/
      â”‚   â””â”€â–º Timestamp: T0
      â”‚
      â”œâ”€â–º S3 triggers Lambda (via event notification)
      â”‚   â””â”€â–º Trigger delay: T1 - T0 (typically < 1 second)
      â”‚
      â”œâ”€â–º Lambda processes file
      â”‚   â”‚
      â”‚   â”œâ”€â–º Download from S3
      â”‚   â”‚   â””â”€â–º Time: ~100-500ms
      â”‚   â”‚
      â”‚   â”œâ”€â–º Parse CSV
      â”‚   â”‚   â””â”€â–º Time: ~10-100ms per 1000 rows
      â”‚   â”‚
      â”‚   â”œâ”€â–º Infer Schema
      â”‚   â”‚   â”‚   â”Œâ”€â–º Integer detection
      â”‚   â”‚   â”‚   â”œâ”€â–º Float detection
      â”‚   â”‚   â”‚   â”œâ”€â–º Date detection
      â”‚   â”‚   â”‚   â””â”€â–º Default to string
      â”‚   â”‚   â””â”€â–º Time: ~50-200ms
      â”‚   â”‚
      â”‚   â”œâ”€â–º Compute Statistics
      â”‚   â”‚   â”‚   â”Œâ”€â–º Min/Max/Avg for numeric columns
      â”‚   â”‚   â”‚   â””â”€â–º Count valid values
      â”‚   â”‚   â””â”€â–º Time: ~50-200ms
      â”‚   â”‚
      â”‚   â”œâ”€â–º Detect Quality Issues
      â”‚   â”‚   â”‚   â”Œâ”€â–º Missing values (% and count)
      â”‚   â”‚   â”‚   â””â”€â–º Invalid values (type mismatches)
      â”‚   â”‚   â””â”€â–º Time: ~50-200ms
      â”‚   â”‚
      â”‚   â”œâ”€â–º Generate Summary JSON
      â”‚   â”‚   â””â”€â–º Time: ~10ms
      â”‚   â”‚
      â”‚   â”œâ”€â–º Upload to S3 processed/
      â”‚   â”‚   â””â”€â–º Time: ~100-300ms
      â”‚   â”‚
      â”‚   â””â”€â–º Write to DynamoDB
      â”‚       â””â”€â–º Time: ~50-150ms
      â”‚
      â””â”€â–º Complete
          â””â”€â–º Total time: T2 - T0 (typically 2-10 seconds)
```

## Processing Logic

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LAMBDA PROCESSOR FLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

START
  â”‚
  â”œâ”€â–º Receive S3 Event
  â”‚   â”‚
  â”‚   â”œâ”€â–º Extract bucket name
  â”‚   â”œâ”€â–º Extract object key
  â”‚   â””â”€â–º Extract timestamp
  â”‚
  â”œâ”€â–º Validate Input
  â”‚   â”‚
  â”‚   â”œâ”€â–º Is key in uploads/? â”€â”€NOâ”€â”€â–º Skip & Return 200
  â”‚   â””â”€â–º Is file .csv? â”€â”€â”€â”€â”€â”€YESâ”€â”€â–º Continue
  â”‚
  â”œâ”€â–º Download File from S3
  â”‚   â”‚
  â”‚   â””â”€â–º Error? â”€â”€YESâ”€â”€â–º Go to Error Handler
  â”‚       â””â”€â”€NOâ”€â”€â–º Continue
  â”‚
  â”œâ”€â–º Parse CSV
  â”‚   â”‚
  â”‚   â”œâ”€â–º Read headers
  â”‚   â”œâ”€â–º Read rows
  â”‚   â””â”€â–º Error? â”€â”€YESâ”€â”€â–º Go to Error Handler
  â”‚       â””â”€â”€NOâ”€â”€â–º Continue
  â”‚
  â”œâ”€â–º Infer Schema (for each column)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Try Integer â”€â”€SUCCESSâ”€â”€â–º Mark as 'int'
  â”‚   â”œâ”€â–º Try Float â”€â”€â”€SUCCESSâ”€â”€â–º Mark as 'float'
  â”‚   â”œâ”€â–º Try Date â”€â”€â”€â”€SUCCESSâ”€â”€â–º Mark as 'date'
  â”‚   â””â”€â–º Default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Mark as 'string'
  â”‚
  â”œâ”€â–º Compute Statistics (for numeric columns)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Calculate min
  â”‚   â”œâ”€â–º Calculate max
  â”‚   â”œâ”€â–º Calculate avg
  â”‚   â””â”€â–º Count valid values
  â”‚
  â”œâ”€â–º Detect Quality Issues
  â”‚   â”‚
  â”‚   â”œâ”€â–º Count missing values (empty cells)
  â”‚   â”œâ”€â–º Count invalid values (type mismatches)
  â”‚   â””â”€â–º Calculate percentages
  â”‚
  â”œâ”€â–º Generate Summary JSON
  â”‚   â”‚
  â”‚   â”œâ”€â–º file_name
  â”‚   â”œâ”€â–º timestamps (upload, processed)
  â”‚   â”œâ”€â–º row_count, column_count
  â”‚   â”œâ”€â–º schema (column types)
  â”‚   â”œâ”€â–º statistics (min/max/avg)
  â”‚   â””â”€â–º quality_issues (missing, invalid)
  â”‚
  â”œâ”€â–º Upload Summary to S3 processed/
  â”‚   â”‚
  â”‚   â””â”€â–º Error? â”€â”€YESâ”€â”€â–º Log warning but continue
  â”‚       â””â”€â”€NOâ”€â”€â–º Continue
  â”‚
  â”œâ”€â–º Write Metadata to DynamoDB
  â”‚   â”‚
  â”‚   â”œâ”€â–º Convert floats to Decimal
  â”‚   â”œâ”€â–º Set status = "success"
  â”‚   â””â”€â–º Put item
  â”‚
  â””â”€â–º Return Success Response
      â””â”€â–º statusCode: 200

ERROR HANDLER
  â”‚
  â”œâ”€â–º Log error to CloudWatch
  â”‚
  â”œâ”€â–º Create error record
  â”‚   â”‚
  â”‚   â”œâ”€â–º file_name
  â”‚   â”œâ”€â–º status = "error"
  â”‚   â”œâ”€â–º error_message
  â”‚   â”œâ”€â–º row_count = 0
  â”‚   â””â”€â–º empty schema/stats
  â”‚
  â”œâ”€â–º Write to DynamoDB
  â”‚
  â””â”€â–º Return Success Response (graceful failure)
      â””â”€â–º statusCode: 200
```

## Deployment Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LOCALSTACK DEPLOYMENT                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

START
  â”‚
  â”œâ”€â–º 1. Start LocalStack Container
  â”‚   â”‚   (docker-compose up -d)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Pull localstack/localstack:latest
  â”‚   â”œâ”€â–º Start container on port 4566
  â”‚   â””â”€â–º Initialize services (S3, Lambda, DynamoDB, IAM, CloudWatch)
  â”‚
  â”œâ”€â–º 2. Initialize Terraform
  â”‚   â”‚   (terraform init)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Download AWS provider
  â”‚   â””â”€â–º Initialize backend
  â”‚
  â”œâ”€â–º 3. Apply Terraform Configuration
  â”‚   â”‚   (terraform apply -var-file="localstack.tfvars")
  â”‚   â”‚
  â”‚   â”œâ”€â–º Create S3 bucket
  â”‚   â”œâ”€â–º Package Lambda code (zip)
  â”‚   â”œâ”€â–º Create Lambda function
  â”‚   â”œâ”€â–º Create DynamoDB table
  â”‚   â”œâ”€â–º Create IAM role (simplified)
  â”‚   â”œâ”€â–º Configure S3 event notification
  â”‚   â””â”€â–º Create CloudWatch log group
  â”‚
  â”œâ”€â–º 4. Test Deployment
  â”‚   â”‚   (python3 test_pipeline.py --env localstack)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Upload test CSV to S3
  â”‚   â”œâ”€â–º Wait for Lambda execution
  â”‚   â”œâ”€â–º Poll DynamoDB for result
  â”‚   â”œâ”€â–º Verify summary JSON in S3
  â”‚   â””â”€â–º Display results
  â”‚
  â””â”€â–º COMPLETE âœ…

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       AWS LEARNER LAB DEPLOYMENT                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

START
  â”‚
  â”œâ”€â–º 1. Configure AWS Credentials
  â”‚   â”‚   (aws configure)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Get credentials from Learner Lab
  â”‚   â”œâ”€â–º Set AWS_ACCESS_KEY_ID
  â”‚   â”œâ”€â–º Set AWS_SECRET_ACCESS_KEY
  â”‚   â”œâ”€â–º Set AWS_SESSION_TOKEN
  â”‚   â””â”€â–º Verify: aws sts get-caller-identity
  â”‚
  â”œâ”€â–º 2. Get Lab Role ARN
  â”‚   â”‚
  â”‚   â””â”€â–º Extract from caller identity or IAM console
  â”‚
  â”œâ”€â–º 3. Initialize Terraform
  â”‚   â”‚   (terraform init)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Download AWS provider
  â”‚   â””â”€â–º Initialize backend
  â”‚
  â”œâ”€â–º 4. Apply Terraform Configuration
  â”‚   â”‚   (terraform apply -var-file="aws.tfvars" -var="lab_role_arn=...")
  â”‚   â”‚
  â”‚   â”œâ”€â–º Create S3 bucket
  â”‚   â”œâ”€â–º Package Lambda code (zip)
  â”‚   â”œâ”€â–º Create Lambda function (using LabRole)
  â”‚   â”œâ”€â–º Create DynamoDB table
  â”‚   â”œâ”€â–º Configure S3 event notification
  â”‚   â””â”€â–º Create CloudWatch log group
  â”‚
  â”œâ”€â–º 5. Test Deployment
  â”‚   â”‚   (python3 test_pipeline.py --env aws)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Upload test CSV to S3
  â”‚   â”œâ”€â–º Wait for Lambda execution
  â”‚   â”œâ”€â–º Poll DynamoDB for result
  â”‚   â”œâ”€â–º Verify summary JSON in S3
  â”‚   â””â”€â–º Display results
  â”‚
  â”œâ”€â–º 6. Collect Metrics
  â”‚   â”‚   (wait 5-10 minutes)
  â”‚   â”‚   (python3 collect_metrics.py --env aws)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Query Lambda metrics
  â”‚   â”œâ”€â–º Query DynamoDB metrics
  â”‚   â”œâ”€â–º Calculate summaries
  â”‚   â””â”€â–º Export to JSON
  â”‚
  â””â”€â–º COMPLETE âœ…
```

## Testing Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         END-TO-END TEST FLOW                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

START TEST
  â”‚
  â”œâ”€â–º 1. Get Terraform Outputs
  â”‚   â”‚
  â”‚   â”œâ”€â–º S3 bucket name
  â”‚   â”œâ”€â–º DynamoDB table name
  â”‚   â””â”€â–º Lambda function name
  â”‚
  â”œâ”€â–º 2. Generate or Load Test Data
  â”‚   â”‚
  â”‚   â”œâ”€â–º Create sample CSV with:
  â”‚   â”‚   â”œâ”€â–º Valid data
  â”‚   â”‚   â”œâ”€â–º Missing values
  â”‚   â”‚   â””â”€â–º Invalid values
  â”‚   â”‚
  â”‚   â””â”€â–º Generate unique filename with timestamp
  â”‚
  â”œâ”€â–º 3. Upload to S3
  â”‚   â”‚
  â”‚   â”œâ”€â–º PUT object to s3://bucket/uploads/test.csv
  â”‚   â”œâ”€â–º Record upload timestamp
  â”‚   â””â”€â–º Print confirmation
  â”‚
  â”œâ”€â–º 4. Poll DynamoDB (max 30 attempts, 2s delay)
  â”‚   â”‚
  â”‚   â”œâ”€â–º Attempt 1 â”€â”€NOT FOUNDâ”€â”€â–º Wait 2s
  â”‚   â”œâ”€â–º Attempt 2 â”€â”€NOT FOUNDâ”€â”€â–º Wait 2s
  â”‚   â”œâ”€â–º ...
  â”‚   â””â”€â–º Attempt N â”€â”€FOUNDâ”€â”€â”€â”€â”€â–º Continue
  â”‚       â””â”€â”€â”€ TIMEOUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º FAIL TEST
  â”‚
  â”œâ”€â–º 5. Retrieve Metadata
  â”‚   â”‚
  â”‚   â”œâ”€â–º Parse DynamoDB item
  â”‚   â””â”€â–º Check status field
  â”‚       â”œâ”€â–º "success" â”€â”€â–º Continue
  â”‚       â””â”€â–º "error" â”€â”€â”€â–º Display error but pass test
  â”‚
  â”œâ”€â–º 6. Verify Summary JSON
  â”‚   â”‚
  â”‚   â”œâ”€â–º Download from s3://bucket/processed/
  â”‚   â”œâ”€â–º Parse JSON
  â”‚   â””â”€â–º Verify structure
  â”‚
  â”œâ”€â–º 7. Calculate Metrics
  â”‚   â”‚
  â”‚   â”œâ”€â–º Total processing time
  â”‚   â”œâ”€â–º Upload â†’ Lambda start delay
  â”‚   â””â”€â–º Lambda execution duration
  â”‚
  â”œâ”€â–º 8. Display Results
  â”‚   â”‚
  â”‚   â”œâ”€â–º File information
  â”‚   â”œâ”€â–º Processing status
  â”‚   â”œâ”€â–º Schema detected
  â”‚   â”œâ”€â–º Statistics computed
  â”‚   â”œâ”€â–º Quality issues found
  â”‚   â””â”€â–º Performance metrics
  â”‚
  â””â”€â–º TEST COMPLETE âœ…

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       METRICS COLLECTION FLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

START COLLECTION
  â”‚
  â”œâ”€â–º 1. Define Time Range
  â”‚   â”‚
  â”‚   â”œâ”€â–º End time: Now
  â”‚   â””â”€â–º Start time: Now - N hours
  â”‚
  â”œâ”€â–º 2. Collect Lambda Metrics
  â”‚   â”‚
  â”‚   â”œâ”€â–º Duration (Average)
  â”‚   â”œâ”€â–º Invocations (Sum)
  â”‚   â”œâ”€â–º Errors (Sum)
  â”‚   â”œâ”€â–º Throttles (Sum)
  â”‚   â””â”€â–º ConcurrentExecutions (Maximum)
  â”‚
  â”œâ”€â–º 3. Collect DynamoDB Metrics
  â”‚   â”‚
  â”‚   â”œâ”€â–º SuccessfulRequestLatency (Average)
  â”‚   â”œâ”€â–º ConsumedWriteCapacityUnits (Sum)
  â”‚   â”œâ”€â–º UserErrors (Sum)
  â”‚   â””â”€â–º SystemErrors (Sum)
  â”‚
  â”œâ”€â–º 4. Calculate Summaries
  â”‚   â”‚
  â”‚   â”œâ”€â–º Min, Max, Average for each metric
  â”‚   â””â”€â–º Total counts where applicable
  â”‚
  â”œâ”€â–º 5. Display Results
  â”‚   â”‚
  â”‚   â”œâ”€â–º Print to console (formatted tables)
  â”‚   â””â”€â–º Export to JSON file
  â”‚
  â””â”€â–º COLLECTION COMPLETE âœ…
```

## Schema Inference Algorithm

```
For each column in CSV:
  â”‚
  â”œâ”€â–º Collect all non-empty values
  â”‚
  â”œâ”€â–º Try INTEGER
  â”‚   â”‚   For each value:
  â”‚   â”‚     Try: int(value)
  â”‚   â”‚   If all succeed â”€â”€â–º TYPE = 'int' â”€â”€â–º DONE
  â”‚   â”‚   If any fail â”€â”€â”€â”€â–º Continue
  â”‚
  â”œâ”€â–º Try FLOAT
  â”‚   â”‚   For each value:
  â”‚   â”‚     Try: float(value)
  â”‚   â”‚   If all succeed â”€â”€â–º TYPE = 'float' â”€â”€â–º DONE
  â”‚   â”‚   If any fail â”€â”€â”€â”€â–º Continue
  â”‚
  â”œâ”€â–º Try DATE
  â”‚   â”‚   For each format in [YYYY-MM-DD, MM/DD/YYYY, ...]:
  â”‚   â”‚     For first 10 values:
  â”‚   â”‚       Try: parse_date(value, format)
  â”‚   â”‚     If all succeed â”€â”€â–º TYPE = 'date' â”€â”€â–º DONE
  â”‚   â”‚     If any fail â”€â”€â”€â”€â–º Try next format
  â”‚   â”‚   If no format works â–º Continue
  â”‚
  â””â”€â–º DEFAULT
      â””â”€â–º TYPE = 'string' â”€â”€â–º DONE
```

## Quality Check Algorithm

```
For each column:
  â”‚
  â”œâ”€â–º Check Missing Values
  â”‚   â”‚   Count = 0
  â”‚   â”‚   For each row:
  â”‚   â”‚     If cell is empty or whitespace only:
  â”‚   â”‚       Count += 1
  â”‚   â”‚   
  â”‚   â”‚   If Count > 0:
  â”‚   â”‚     Record: {count, percentage}
  â”‚   â”‚     Set has_issues = True
  â”‚
  â””â”€â–º Check Invalid Values (if column is int or float)
      â”‚   Count = 0
      â”‚   For each non-empty value:
      â”‚     Try to convert to column type
      â”‚     If conversion fails:
      â”‚       Count += 1
      â”‚   
      â”‚   If Count > 0:
      â”‚     Record: {count, percentage, expected_type}
      â”‚     Set has_issues = True

Return: {
  total_rows,
  missing_values: {column: {count, percentage}},
  invalid_values: {column: {count, percentage, expected_type}},
  has_issues: boolean
}
```

## User Journey

```
Developer Setup:
  1. Clone repository
  2. Read README.md
  3. Install prerequisites (Terraform, AWS CLI, Python, Docker)
  4. Start LocalStack: make localstack-up
  5. Deploy: make deploy-localstack
  6. Test: make test-localstack
  7. Iterate on code, redeploy, test

Production Deployment:
  1. Access AWS Learner Lab
  2. Configure credentials: aws configure
  3. Deploy: make deploy-aws
  4. Test: make test-aws
  5. Monitor: make metrics-aws

Daily Usage:
  1. Upload CSV files to S3 uploads/
  2. Files are automatically processed
  3. Check DynamoDB for status
  4. Download summary JSON from S3 processed/
  5. Review CloudWatch logs if issues

Monitoring:
  1. Run: python3 collect_metrics.py --env aws
  2. Review metrics JSON
  3. Check CloudWatch dashboard
  4. Analyze performance trends
  5. Optimize if needed

Cleanup:
  1. LocalStack: make clean-localstack
  2. AWS: make clean-aws
  3. Stop LocalStack: docker-compose down
```

## State Transitions

```
File Upload â†’ Processing â†’ Complete/Error

States:
  UPLOADED â”€â”€Lambda Triggeredâ”€â”€â–º PROCESSING
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                  â”‚
            Processing Success                Processing Error
                    â”‚                                  â”‚
                    â–¼                                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  DynamoDB Record    â”‚            â”‚  DynamoDB Record    â”‚
       â”‚  status="success"   â”‚            â”‚  status="error"     â”‚
       â”‚  + full metadata    â”‚            â”‚  + error_message    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                  â”‚
                    â”‚                                  â”‚
                    â–¼                                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  S3 Summary JSON    â”‚            â”‚  No summary JSON    â”‚
       â”‚  in processed/      â”‚            â”‚  created            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Navigation**:
- ğŸ“– **Overview**: `README.md`
- ğŸš€ **Deploy**: `DEPLOYMENT.md`
- ğŸ§ª **Test**: `TESTING.md`
- ğŸ—ï¸ **Architecture**: `ARCHITECTURE.md`
- ğŸ“‹ **Summary**: `PROJECT_SUMMARY.md`
- âš¡ **Quick Ref**: `QUICK_REFERENCE.md`
- ğŸ”„ **Workflow**: This file

